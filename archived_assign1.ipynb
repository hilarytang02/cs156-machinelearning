{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: explain the data\n",
    "- what is included\n",
    "- how it was obtained\n",
    "- all important details about how it was sampled from the student's own digital archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description**\n",
    "\n",
    "\"if you want to know what someone fears losing watch what they photograph.\"\n",
    "\n",
    "One of my hobbies is taking photos with\n",
    "I love to record how I see the world through camera, especially film and ccd, so I have a collection of 1000+ pictures in my computer. \n",
    "-> some are sghare but some are only downloaded on to the phone and --> so diff in resolution (remove if not necessary)\n",
    "\n",
    "Something special about taking photos with antique cameras There is a more grain in --> so requires extra processing to increase contrast fopr better processing???? \n",
    "\n",
    "I removed the film pictures that are overexposed as well as dispoable camera that i used when i first started taking pics\n",
    "try to only to use pictures from one camera (nikon lite touch zoom 110w)\n",
    "but since i like to experiment different brands and types of film --> the colour gradient is diverse\n",
    "\n",
    "\n",
    "**Overall Objective**\n",
    "\n",
    "\n",
    "**Objective for draft 1**\n",
    "\n",
    "Build a simple image classification model to differentiate human portraits from non-human portraits in film photography.\n",
    "\n",
    "A human portrait is an image where a human is the central and dominant subject, with a clear emphasis on their face, expression, or body (e.g., a headshot, half-body, or full-body portrait).\n",
    "An image is not a portrait if the human is small in the background or if other elements (landscape, architecture, etc.) dominate the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Data Ingestion\n",
    "- converting this data to Python readable format (and sci-kit-learn) \n",
    "- loading this data into an appropriate data structure (np.array, pandas dataframe, glob etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Define the paths to your image directories\n",
    "portrait_dir = \"data/portraits/\"  # Folder containing portrait images\n",
    "non_portrait_dir = \"data/non_portraits/\"  # Folder containing non-portrait images\n",
    "\n",
    "# Function to load images from a given folder\n",
    "def load_images_from_folder(folder, label, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads images from a specified folder, resizes them to a fixed size, \n",
    "    and converts them to NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str): Path to the folder containing images.\n",
    "    - label (int): Label assigned to images (1 for portraits, 0 for non-portraits).\n",
    "    - target_size (tuple): Desired image dimensions (default is 224x224 pixels).\n",
    "\n",
    "    Returns:\n",
    "    - images (list): List of image arrays.\n",
    "    - labels (list): List of corresponding labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through all image files in the folder\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)  # Get the full file path\n",
    "        \n",
    "        try:\n",
    "            with Image.open(img_path) as img:  # Open the image\n",
    "                img = img.resize(target_size)  # Resize image to maintain consistency\n",
    "                img_array = np.array(img)  # Convert image to a NumPy array\n",
    "                \n",
    "                images.append(img_array)  # Append processed image to list\n",
    "                labels.append(label)  # Append corresponding label (1 or 0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")  # Print error message if loading fails\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Load images and labels from both folders\n",
    "portrait_images, portrait_labels = load_images_from_folder(portrait_dir, label=1)  # Load portraits\n",
    "non_portrait_images, non_portrait_labels = load_images_from_folder(non_portrait_dir, label=0)  # Load non-portraits\n",
    "\n",
    "# Combine portrait and non-portrait data\n",
    "X = np.array(portrait_images + non_portrait_images)  # Stack all image arrays\n",
    "y = np.array(portrait_labels + non_portrait_labels)  # Stack all labels\n",
    "\n",
    "# Shuffle dataset to randomize image order\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Function to display sample images\n",
    "def show_sample_images(X, y, num_samples=5):\n",
    "    \"\"\"\n",
    "    Displays a few sample images from the dataset with labels.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy array): Array of image data.\n",
    "    - y (numpy array): Array of labels (1 = portrait, 0 = non-portrait).\n",
    "    - num_samples (int): Number of sample images to display (default is 5).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))  # Set figure size\n",
    "    for i in range(num_samples):\n",
    "        ax = plt.subplot(1, num_samples, i + 1)  # Create subplots\n",
    "        plt.imshow(X[i])  # Show the image\n",
    "        plt.title(\"Portrait\" if y[i] == 1 else \"Non-Portrait\")  # Assign title based on label\n",
    "        plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "# Display 5 sample images to verify the dataset\n",
    "show_sample_images(X, y, num_samples=5)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Total images loaded: {X.shape[0]}\")  # Number of images loaded\n",
    "print(f\"Image dimensions: {X.shape[1:]}\")  # Shape of each image (Height, Width, Channels)\n",
    "print(f\"Label distribution: {np.unique(y, return_counts=True)}\")  # Count of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Data Cleaning \n",
    "explaining any necessary cleaning, pre-processing, and feature engineering the data requires\n",
    "\n",
    "+ [code block] completing these steps. \n",
    "perform some basic exploratory data analysis at this point reporting and visualizing the samples and computing appropriate descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2  # For image processing\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# Set image directories\n",
    "portrait_dir = \"path_to_your_images/portraits/\"\n",
    "non_portrait_dir = \"path_to_your_images/non_portraits/\"\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (224, 224)  # Standard CNN input size\n",
    "USE_GRAYSCALE = False  # Set to True if you want grayscale images\n",
    "APPLY_AUGMENTATION = True  # Apply data augmentation\n",
    "\n",
    "def preprocess_image(img_path, target_size=IMG_SIZE, grayscale=USE_GRAYSCALE, augment=APPLY_AUGMENTATION):\n",
    "    \"\"\"\n",
    "    Loads an image, resizes it, applies optional grayscale conversion,\n",
    "    normalizes pixel values, and performs data augmentation if enabled.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_path (str): Path to the image file.\n",
    "    - target_size (tuple): The desired image size (width, height).\n",
    "    - grayscale (bool): Convert to grayscale if True.\n",
    "    - augment (bool): Apply data augmentation if True.\n",
    "\n",
    "    Returns:\n",
    "    - Processed image as a NumPy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image using PIL\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Convert to grayscale if required\n",
    "        if grayscale:\n",
    "            img = img.convert(\"L\")  # Convert to grayscale\n",
    "        \n",
    "        # Resize image\n",
    "        img = img.resize(target_size)\n",
    "\n",
    "        # Apply data augmentation\n",
    "        if augment:\n",
    "            img = apply_augmentation(img)\n",
    "\n",
    "        # Convert to NumPy array\n",
    "        img_array = np.array(img)\n",
    "\n",
    "        # Normalize pixel values to range [0,1]\n",
    "        img_array = img_array / 255.0\n",
    "\n",
    "        return img_array\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def apply_augmentation(img):\n",
    "    \"\"\"\n",
    "    Applies random augmentations to the given image to improve model generalization.\n",
    "    - Random rotation\n",
    "    - Random brightness adjustment\n",
    "    - Random horizontal flipping\n",
    "    - Gaussian noise (to simulate film grain)\n",
    "\n",
    "    Parameters:\n",
    "    - img (PIL.Image): The input image.\n",
    "\n",
    "    Returns:\n",
    "    - Augmented image.\n",
    "    \"\"\"\n",
    "    # Random rotation (-15 to +15 degrees)\n",
    "    angle = random.uniform(-15, 15)\n",
    "    img = img.rotate(angle)\n",
    "\n",
    "    # Random brightness adjustment (80% to 120% brightness)\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    brightness_factor = random.uniform(0.8, 1.2)\n",
    "    img = enhancer.enhance(brightness_factor)\n",
    "\n",
    "    # Random horizontal flip\n",
    "    if random.random() > 0.5:  # 50% chance\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    # Convert to NumPy for Gaussian noise\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Apply Gaussian noise (simulates film grain)\n",
    "    noise = np.random.normal(loc=0, scale=0.02, size=img_np.shape)  # Small noise\n",
    "    img_np = np.clip(img_np + noise * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return Image.fromarray(img_np)  # Convert back to PIL format\n",
    "\n",
    "# Load and preprocess all images\n",
    "def load_and_preprocess_images(folder, label):\n",
    "    \"\"\"\n",
    "    Loads all images from a folder, preprocesses them, and stores them in arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str): Path to the folder containing images.\n",
    "    - label (int): Label for classification (1 = portrait, 0 = non-portrait).\n",
    "\n",
    "    Returns:\n",
    "    - images (list): List of preprocessed images as NumPy arrays.\n",
    "    - labels (list): Corresponding labels.\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        processed_img = preprocess_image(img_path)\n",
    "        \n",
    "        if processed_img is not None:\n",
    "            images.append(processed_img)\n",
    "            labels.append(label)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load and preprocess portrait and non-portrait images\n",
    "portrait_images, portrait_labels = load_and_preprocess_images(portrait_dir, label=1)\n",
    "non_portrait_images, non_portrait_labels = load_and_preprocess_images(non_portrait_dir, label=0)\n",
    "\n",
    "# Combine dataset\n",
    "X = np.array(portrait_images + non_portrait_images)  # Image data\n",
    "y = np.array(portrait_labels + non_portrait_labels)  # Labels\n",
    "\n",
    "# Shuffle dataset\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Display some preprocessed images\n",
    "def show_preprocessed_samples(X, y, num_samples=5):\n",
    "    \"\"\"\n",
    "    Displays a few sample images from the preprocessed dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy array): Preprocessed image data.\n",
    "    - y (numpy array): Labels (1 = portrait, 0 = non-portrait).\n",
    "    - num_samples (int): Number of images to display.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_samples):\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(X[i] if not USE_GRAYSCALE else X[i].squeeze(), cmap=\"gray\" if USE_GRAYSCALE else None)\n",
    "        plt.title(\"Portrait\" if y[i] == 1 else \"Non-Portrait\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show 5 sample preprocessed images\n",
    "show_preprocessed_samples(X, y, num_samples=5)\n",
    "\n",
    "# Print dataset details\n",
    "print(f\"Total images processed: {X.shape[0]}\")\n",
    "print(f\"Image shape: {X.shape[1:]}\")  # Expected: (224, 224, 3) or (224, 224) for grayscale\n",
    "print(f\"Label distribution: {np.unique(y, return_counts=True)}\")  # Check class balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Analysis\n",
    "- discussing the analysis (classification, regression, or clustering) that will be conducted on the data\n",
    "- code performs any necessary data splits (such as creating training and test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [well commented code block] performs any necessary data splits (such as creating training and test sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Model Selection\n",
    "\n",
    "- discuss model** selection** in a markdown section \n",
    "- [code] model initialization and construction in a well-commented code block. \n",
    "- This section should include a clear discussion of the model's mathematical underpinnings -> include typeset equations and/or algorithms as pseudocode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[draft]\n",
    "To establish a benchmark for performance, we first consider a baseline model using Logistic Regression or Support Vector Machines (SVM). These models are widely used for classification tasks but are not typically applied to raw image data due to the high dimensionality. However, using them as a starting point allows us to quantify the challenge of the classification task before introducing deep learning models.\n",
    "\n",
    "For this approach, images are flattened into one-dimensional vectors, where each pixel becomes a feature in a traditional machine learning model. Despite its limitations, this method provides insights into whether raw pixel values alone carry enough information for classification. If the baseline performs poorly, it indicates that a more advanced feature extraction approach, such as convolutional neural networks (CNNs), is necessary. The key advantage of this baseline approach is its simplicity—training a logistic regression or SVM model is computationally inexpensive and provides an interpretable reference point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: train the model\n",
    "- code and explanations for necessary cross validation or hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: \n",
    "- code to generate predictions for out of sample data, and compute appropriate performance** metrics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: visualize the results and discuss your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: executive summary (of the prior eight sections)\n",
    "- clearly explaining your steps\n",
    "- diagramming your pipeline\n",
    "- visualizing any key results\n",
    "- explaining any key insights or shortcomings of your approach. \n",
    "- You may wish to include a discussion of how the model might be improved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: references \n",
    "- documents, guides, or code repos you accessed for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Feature Extraction Using BERT Embeddings\n",
    "\n",
    "We generate embeddings for each song's lyrics using a BERT model. These embeddings will be used for similarity comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Feature Extraction Using BERT Embeddings\n",
    "\n",
    "We generate embeddings for each song's lyrics using a BERT model. These embeddings will be used for similarity comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# We generate embeddings for each song's lyrics using a BERT model. These embeddings will be used for similarity comparisons.\n",
    "\n",
    "# Initialize the tokenizer and model (using the base uncased model)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    # If 'text' is a list of tokens, join them into a single string\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get all token embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    # Mean pooling over the sequence dimension; detach from graph and move to CPU\n",
    "    mean_embedding_tensor = torch.mean(embeddings, dim=1).squeeze().cpu().detach()\n",
    "    # Convert tensor to list then to a NumPy array\n",
    "    mean_embedding = np.array(mean_embedding_tensor.tolist())\n",
    "    return mean_embedding\n",
    "\n",
    "# Example: Generate BERT embeddings for each song in the DataFrame\n",
    "# Assume 'df' contains a 'lyrics' column with the cleaned lyric text.\n",
    "df['bert_embedding'] = df['cleaned_tokens'].apply(get_bert_embedding)\n",
    "\n",
    "# Convert embeddings into a feature matrix (stacking arrays)\n",
    "X_bert = np.vstack(df['bert_embedding'].values)\n",
    "print(\"BERT Embedding shape:\", X_bert.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Step 5: Query Representation, Similarity Calculation, and Ranking\n",
    "# In this section we restrict the input to one imagery-related word (e.g. \"sunset\"), generate its embedding, compute cosine similarity with each song's embedding, and rank the songs.\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_query_embedding(query_word):\n",
    "    \"\"\"\n",
    "    Process the input query word (or phrase) to generate a BERT embedding.\n",
    "    Since the input is restricted to an imagery-related word, we split and process it.\n",
    "    \"\"\"\n",
    "    tokens = query_word.split()  # For a single word or a simple phrase\n",
    "    return get_bert_embedding(tokens)\n",
    "\n",
    "# Set your imagery-related query word here\n",
    "query = \"apple\"  # You can change this as needed\n",
    "\n",
    "# Generate the embedding for the query word\n",
    "query_embedding = get_query_embedding(query)\n",
    "\n",
    "# Reshape the query embedding for cosine similarity computation\n",
    "query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarities between the query and all song embeddings\n",
    "similarities = cosine_similarity(query_embedding, X_bert).flatten()\n",
    "\n",
    "# Add the similarity scores to the DataFrame\n",
    "df['similarity'] = similarities\n",
    "\n",
    "# Rank the songs by similarity (descending order)\n",
    "df_sorted = df.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "print(f\"\\nTop ranked songs for query '{query}':\")\n",
    "print(df_sorted[['filename', 'similarity']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, visualize the distribution of similarity scores\n",
    "import seaborn as sns\n",
    "sns.histplot(similarities, kde=True)\n",
    "plt.title(f\"Cosine Similarity Distribution for Query: '{query}'\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# We generate embeddings for each song's lyrics using a BERT model. These embeddings will be used for similarity comparisons.\n",
    "\n",
    "# Initialize the tokenizer and model (using the base uncased model)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    # If 'text' is a list of tokens, join them into a single string\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get all token embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    # Mean pooling over the sequence dimension; detach from graph and move to CPU\n",
    "    mean_embedding_tensor = torch.mean(embeddings, dim=1).squeeze().cpu().detach()\n",
    "    # Convert tensor to list then to a NumPy array\n",
    "    mean_embedding = np.array(mean_embedding_tensor.tolist())\n",
    "    return mean_embedding\n",
    "\n",
    "# Example: Generate BERT embeddings for each song in the DataFrame\n",
    "# Assume 'df' contains a 'lyrics' column with the cleaned lyric text.\n",
    "df['bert_embedding'] = df['cleaned_tokens'].apply(get_bert_embedding)\n",
    "\n",
    "# Convert embeddings into a feature matrix (stacking arrays)\n",
    "X_bert = np.vstack(df['bert_embedding'].values)\n",
    "print(\"BERT Embedding shape:\", X_bert.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Step 5: Query Representation, Similarity Calculation, and Ranking\n",
    "# In this section we restrict the input to one imagery-related word (e.g. \"sunset\"), generate its embedding, compute cosine similarity with each song's embedding, and rank the songs.\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_query_embedding(query_word):\n",
    "    \"\"\"\n",
    "    Process the input query word (or phrase) to generate a BERT embedding.\n",
    "    Since the input is restricted to an imagery-related word, we split and process it.\n",
    "    \"\"\"\n",
    "    tokens = query_word.split()  # For a single word or a simple phrase\n",
    "    return get_bert_embedding(tokens)\n",
    "\n",
    "# Set your imagery-related query word here\n",
    "query = \"apple\"  # You can change this as needed\n",
    "\n",
    "# Generate the embedding for the query word\n",
    "query_embedding = get_query_embedding(query)\n",
    "\n",
    "# Reshape the query embedding for cosine similarity computation\n",
    "query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarities between the query and all song embeddings\n",
    "similarities = cosine_similarity(query_embedding, X_bert).flatten()\n",
    "\n",
    "# Add the similarity scores to the DataFrame\n",
    "df['similarity'] = similarities\n",
    "\n",
    "# Rank the songs by similarity (descending order)\n",
    "df_sorted = df.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "print(f\"\\nTop ranked songs for query '{query}':\")\n",
    "print(df_sorted[['filename', 'similarity']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, visualize the distribution of similarity scores\n",
    "import seaborn as sns\n",
    "sns.histplot(similarities, kde=True)\n",
    "plt.title(f\"Cosine Similarity Distribution for Query: '{query}'\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_lyrics(text):\n",
    "    # 1. Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    # 2. Remove the first line (if you always want to discard it unconditionally)\n",
    "    if len(lines) > 0:\n",
    "        lines = lines[1:]\n",
    "    # 3. Remove lines containing square brackets (e.g. [Chorus], [Verse 1])\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        # If the line does not contain square brackets, keep it\n",
    "        if not re.search(r'\\[.*?\\]', line):\n",
    "            filtered_lines.append(line)\n",
    "    # 4. Rejoin the remaining lines into a single string\n",
    "    cleaned_text = ' '.join(filtered_lines)\n",
    "    # 5. Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    # 6. Remove punctuation (keep spaces and word characters only)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    # 7. Tokenize the text\n",
    "    tokens = nltk.word_tokenize(cleaned_text)\n",
    "    # 8. Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # 9. Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the lyrics column and create a new column for cleaned tokens\n",
    "df['cleaned_tokens'] = df['lyrics'].apply(preprocess_lyrics)\n",
    "\n",
    "# Add a column for the length of the lyrics (number of tokens)\n",
    "df['num_tokens'] = df['cleaned_tokens'].apply(len)\n",
    "\n",
    "print(\"\\nData Cleaning: DataFrame after preprocessing\")\n",
    "print(df[['filename', 'num_tokens']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
